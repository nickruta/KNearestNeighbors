{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors Example and Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from scipy.spatial import distance\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ronald Fisher's Iris Data Set \n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We split the iris dataset into training data and test data.\n",
    "np.random.seed(1000)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_data_train = iris_X[indices[:-20]]\n",
    "iris_target_train = iris_Y[indices[:-20]]\n",
    "iris_data_test  = iris_X[indices[-20:]]\n",
    "iris_target_test  = iris_Y[indices[-20:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn's Built in k-NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 2 1 0 1 0 0 2 2 0 0 1 2 0 1 1 1 1]\n",
      "[1 1 0 2 1 0 1 0 0 2 2 0 0 1 2 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Basic configuration of the sklearn k-NN classifier.\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', p=2, metric='minkowski')\n",
    "knn.fit(iris_data_train, iris_target_train)\n",
    "\n",
    "print(knn.predict(iris_data_test))\n",
    "print(iris_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class knearestbasic():\n",
    "    \"\"\"\n",
    "    Basic implementation of the k-Nearest Neighbor classification algorithm\n",
    "    as described in https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data, training_targets):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with the given training data and\n",
    "        classes.\n",
    "        \"\"\"\n",
    "        self.training_data = training_data\n",
    "        self.training_targets = training_targets\n",
    "        \n",
    "        self._classes = set(training_targets)\n",
    "        self._num_classes = len(self._classes)\n",
    "        \n",
    "        self._features = len(training_data[0])\n",
    "        \n",
    "    def classify(self, cases, k=5, dist_func=distance.euclidean):\n",
    "        \"\"\"\n",
    "        Classify a set of cases using training data.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for case in cases:\n",
    "            neighbors = self.__get_neighbors(case, k, dist_func)\n",
    "            vote = self.__count_votes(neighbors)[0][0]\n",
    "            results.append(vote)\n",
    "        return results\n",
    "    \n",
    "    def __get_neighbors(self, case, k, dist_func):\n",
    "        \"\"\"\n",
    "        Classifies a single case using training data.\n",
    "        \"\"\"\n",
    "        if (len(case) != self._features):\n",
    "            raise ValueError(\"invalid case\")\n",
    "            \n",
    "        distances = []\n",
    "        for i in range(len(self.training_data)):\n",
    "            dist = dist_func(case, self.training_data[i])\n",
    "            distances.append([self.training_targets[i], dist])\n",
    "\n",
    "        distances.sort(key=itemgetter(1))\n",
    "        return distances[0:k]\n",
    "    \n",
    "    def __count_votes(self, neighbors):\n",
    "        \"\"\"\n",
    "        Counts votes for classes and handles ties.\n",
    "        \"\"\"\n",
    "        votes = {}\n",
    "        for i in range(len(neighbors)):\n",
    "            cl = neighbors[i][0]\n",
    "            if cl in votes:\n",
    "                votes[cl] += 1\n",
    "            else:\n",
    "                votes[cl] = 1\n",
    "        sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "        # Break ties by counting fewer neighbors\n",
    "        if len(sorted_votes) > 1 and sorted_votes[0][1] == sorted_votes[1][1]:\n",
    "            return self.__count_votes(neighbors[:-1])\n",
    "\n",
    "        return sorted_votes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 2 1 0 1 0 0 2 2 0 0 1 2 0 1 1 1 1]\n",
      "[1 1 0 2 1 0 1 0 0 2 2 0 0 1 2 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "knn = knearestbasic(iris_data_train, iris_target_train)\n",
    "print(str(knn.classify(iris_data_test, 10)).replace(',',''))\n",
    "print(iris_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Measuring Quality\n",
    "Since we are interested in the quality as well as the accuracy of our classifying methods, we include mechanisms for calculating the Degree of Certainty and Net reliability of our classifiers - demonstrated using the Variable K Nearest Neightbor Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "def degree_of_certainty(scores):\n",
    "    \"\"\"\n",
    "    Calculates the degree of certainty given a set of classification\n",
    "    scores. In our model, these are the results returned by the\n",
    "    __get_neighbors function. e.g. List of [class, classification_score]\n",
    "    \"\"\"\n",
    "    max_score = 0\n",
    "    total = 0\n",
    "    for i in range(len(scores)):\n",
    "        if (scores[i][1] > max_score):\n",
    "            max_score = scores[i][1]\n",
    "        total += scores[i][1]\n",
    "        \n",
    "    return max_score/total\n",
    "    \n",
    "    \n",
    "test_scores = [(2, 8), (1, 2), (0, 1)]\n",
    "print(degree_of_certainty(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable k Nearest\n",
    "\n",
    "This classifier uses the degree of certainty to compute the ideal value of k for a given training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class variableknn():\n",
    "    \"\"\"\n",
    "    Implementation of the variable KNN detection algorithm. Which precomputes\n",
    "    the ideal k value for your training set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data, training_targets, dist_func=distance.euclidean):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with the given training data and\n",
    "        classes.\n",
    "        \"\"\"\n",
    "        self.training_data = training_data\n",
    "        self.training_targets = training_targets\n",
    "        self.dist_func = dist_func\n",
    "        \n",
    "        self._classes = set(training_targets)\n",
    "        self._num_classes = len(self._classes)\n",
    "        \n",
    "        self._features = len(training_data[0])\n",
    "        \n",
    "        # Precalculate optimum k values\n",
    "        self.__calc_k_values()\n",
    "   \n",
    "\n",
    "    def classify(self, cases):\n",
    "        results = []\n",
    "        for case in cases:\n",
    "            neighbors = self.__get_neighbors(case, self._k, False)\n",
    "            vote = self.__class_scores(neighbors)[0][0]\n",
    "            results.append(vote)\n",
    "        return results\n",
    "    \n",
    "            \n",
    "    def __calc_k_values(self):\n",
    "        \"\"\"\n",
    "        For each of the training set elements a classification of it is performed based on a\n",
    "        certain k value. We then find the average DC for each of those points and maximize that\n",
    "        w.r.t. a changing k.\n",
    "        \n",
    "        To keep this simple we're just checking each k value for 1 < k < 30.\n",
    "        \"\"\"\n",
    "        self._k = {cl : 0 for cl in set(self.training_targets)}\n",
    "        for i in range(5, 30):\n",
    "            self.__check_k(i)\n",
    "        #print(self._k)\n",
    "        \n",
    "        \n",
    "    def __check_k(self, k):\n",
    "        \"\"\"\n",
    "        This function computes the average degree of certainty for each element in the set\n",
    "        given a certain k.\n",
    "        \"\"\"\n",
    "        dc_scores = {}\n",
    "        dc_counts = {}\n",
    "        for i in range(len(self.training_data)):\n",
    "            if self.training_targets[i] in dc_counts:\n",
    "                dc_counts[self.training_targets[i]] += 1\n",
    "            else:\n",
    "                dc_counts[self.training_targets[i]] = 1\n",
    "                dc_scores[self.training_targets[i]] = 0\n",
    "                \n",
    "            neighbors = self.__get_neighbors(self.training_data[i], k, True)\n",
    "            scores = self.__class_scores(neighbors)\n",
    "            dc = degree_of_certainty(scores)\n",
    "            dc_scores[self.training_targets[i]] += dc\n",
    "            \n",
    "        for cl in dc_counts:\n",
    "            dc_scores[cl] = dc_scores[cl]/dc_counts[cl]\n",
    "            if self._k[cl] < dc_scores[cl]:\n",
    "                self._k[cl] = k\n",
    "        #print(dc_scores)\n",
    "    \n",
    "    \n",
    "    def __get_neighbors(self, case, k, skip):\n",
    "        if (len(case) != self._features):\n",
    "            raise ValueError(\"invalid case\")\n",
    "            \n",
    "        distances = []\n",
    "        \n",
    "        for i in range(len(self.training_data)):\n",
    "            dist = self.dist_func(case, self.training_data[i])\n",
    "            if dist == 0 and skip:\n",
    "                continue\n",
    "            distances.append([self.training_targets[i], dist])\n",
    "\n",
    "        distances.sort(key=itemgetter(1))\n",
    "        \n",
    "        if skip:\n",
    "            # For precalculation\n",
    "            return distances[0:k]\n",
    "        else:\n",
    "            # Otherwise lookup the optimal value of k for the nearest neighbor, then\n",
    "            # use that value.\n",
    "            opt_k = self._k[distances[0][0]]\n",
    "            return distances[0:opt_k]\n",
    "    \n",
    "    def __class_scores(self, neighbors):\n",
    "        votes = {}\n",
    "        for i in range(len(neighbors)):\n",
    "            cl = neighbors[i][0]\n",
    "            if cl in votes:\n",
    "                votes[cl] += 1\n",
    "            else:\n",
    "                votes[cl] = 1\n",
    "        sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "        # Break ties by counting fewer neighbors\n",
    "        if len(sorted_votes) > 1 and sorted_votes[0][1] == sorted_votes[1][1]:\n",
    "            return self.__class_scores(neighbors[:-1])\n",
    "        \n",
    "        return sorted_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = variableknn(iris_data_train, iris_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(knn.classify(iris_data_test)).replace(',',''))\n",
    "print(iris_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Data Sets\n",
    "\n",
    "Looking at classifying some other data sets as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
